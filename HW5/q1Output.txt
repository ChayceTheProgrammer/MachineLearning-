
DEMONSTRATING EXPONENTIAL GROWTH IN AUTOREGRESSIVE LANGUAGE MODELS
======================================================================
For vocabulary size V = 5:
+---------------------------+---------------------+
| Conditional Probability   | Number of Entries   |
+===========================+=====================+
| p(x_1|x_1,...,x_{0}))     | 5                   |
+---------------------------+---------------------+
| p(x_2|x_1,...,x_{1}))     | 25                  |
+---------------------------+---------------------+
| p(x_3|x_1,...,x_{2}))     | 125                 |
+---------------------------+---------------------+
| p(x_4|x_1,...,x_{3}))     | 625                 |
+---------------------------+---------------------+
| p(x_5|x_1,...,x_{4}))     | 3,125               |
+---------------------------+---------------------+
| p(x_6|x_1,...,x_{5}))     | 15,625              |
+---------------------------+---------------------+
| p(x_7|x_1,...,x_{6}))     | 78,125              |
+---------------------------+---------------------+
| p(x_8|x_1,...,x_{7}))     | 390,625             |
+---------------------------+---------------------+
| Total                     | 488,280             |
+---------------------------+---------------------+

Note: Formula verification: 488,280 = 488,280
The formula is: V * (1 - V^N) / (1 - V)

Generating plot to visualize the exponential growth...
Plot saved as 'autoregressive_growth.png'

PRACTICAL EXAMPLE
For GPT models with vocabulary size of ~50,000 tokens:
For a 2-gram model (n=2): 2,500,000,000 entries needed
For a 3-gram model (n=3): 125,000,000,000,000 entries needed
For a context of 10 tokens: 97,656,250,000,000,007,450,947,110,441,295,901,655,239,229,440 entries needed
For a context of 100 tokens: Too large to compute (more than atoms in the universe)

This is why neural networks are used to parameterize these distributions
instead of explicit probability tables.

Process finished with exit code 0